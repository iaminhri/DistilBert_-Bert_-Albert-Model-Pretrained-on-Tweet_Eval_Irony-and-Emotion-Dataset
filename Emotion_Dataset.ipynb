{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# from huggingface_hub import notebook_login\n",
        "# notebook_login()"
      ],
      "metadata": {
        "id": "So3k393KQeHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !apt install git-lfs"
      ],
      "metadata": {
        "id": "dpytF0hHQg8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DistilBert Model Text Classification"
      ],
      "metadata": {
        "id": "XPx0PWFQxlP1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEoqXkPm2aRR"
      },
      "outputs": [],
      "source": [
        "# importing libaries\n",
        "!pip install -U accelerate\n",
        "!pip install -U transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMfecQ7FGtVB"
      },
      "outputs": [],
      "source": [
        "# importing datasets and tokenizer.\n",
        "!pip install datasets\n",
        "from datasets import load_dataset\n",
        "from transformers import DistilBertTokenizer\n",
        "\n",
        "# loading dataset\n",
        "dataset = load_dataset('dair-ai/emotion')\n",
        "\n",
        "# loading pre-trained model bert\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# Tokenize function takes a dataset as input, pads it based on max_length and truncates if above max_length.\n",
        "def tokenize_function(example):\n",
        "  return tokenizer(example[\"text\"], padding=True, truncation=True, max_length=512)\n",
        "\n",
        "# applying tokenized function on tha dataset in batches.\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bGGhyXeHrUpv"
      },
      "outputs": [],
      "source": [
        "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# loading the pre-trained weight of distilbert for sequence classification and initialize a model with two labels.\n",
        "def model_init():\n",
        "  return DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BE-dR-NPs3-L"
      },
      "outputs": [],
      "source": [
        "model_name = \"distilbert-finetuned-emotion\"\n",
        "\n",
        "# Training arguments.\n",
        "training_args = TrainingArguments(\n",
        "  output_dir=model_name, # Directory for saving outputs\n",
        "  learning_rate=2.8743538823133815e-05, # Learning rate for optimization\n",
        "  seed = 1, # num of random seeds\n",
        "  per_device_train_batch_size=4, # Batch size for training\n",
        "  per_device_eval_batch_size=64, # Batch size for evaluation\n",
        "  num_train_epochs=3, # Number of training epochs\n",
        "  weight_decay=0.01, # Weight decay for regularization\n",
        "  evaluation_strategy=\"epoch\", # Evaluation is done at the end of each epoch\n",
        "  save_strategy = \"epoch\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6RDpYXo14MQ"
      },
      "outputs": [],
      "source": [
        "# Trainer Initialization using training pipeline from huggingface.\n",
        "trainer = Trainer(\n",
        "  model_init=model_init,\n",
        "  args=training_args,\n",
        "  train_dataset=tokenized_datasets['train'],\n",
        "  eval_dataset=tokenized_datasets['validation'],\n",
        "  compute_metrics=lambda p: {\"accuracy\": accuracy_score(p.label_ids,\n",
        "  np.argmax(p.predictions, axis=1))},\n",
        "  tokenizer=tokenizer\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting the test, train, and validation dataset.\n",
        "\n",
        "test_dataset = tokenized_datasets['test']\n",
        "eval_dataset = tokenized_datasets['validation']\n",
        "training_set = tokenized_datasets['train']\n",
        "\n",
        "# Untrained state evaluation.\n",
        "eval_training = trainer.evaluate(training_set)\n",
        "eval_test = trainer.evaluate(test_dataset)\n",
        "eval_validation = trainer.evaluate(eval_dataset)\n",
        "\n",
        "# Printing untrained state accuracy\n",
        "print(\"Training: \", eval_training)\n",
        "print(\"Testing: \", eval_test)\n",
        "print(\"Validation: \", eval_validation)"
      ],
      "metadata": {
        "id": "s7m7Frhw9spG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicted emotions before training:\n",
        "from transformers import DistilBertTokenizer, DistilBertConfig, DistilBertModel\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# loads the DistilBert model, and initilizes config, sequence classification.\n",
        "model_path = 'distilbert-base-uncased'\n",
        "config = DistilBertConfig.from_pretrained(model_path)\n",
        "model_saved = DistilBertForSequenceClassification.from_pretrained(model_path, config=config)\n",
        "\n",
        "# first 5 test instances.\n",
        "inputs = test_dataset['text'][:5]\n",
        "\n",
        "# returns as py torch sequences using the tokenizer.\n",
        "input_ids = tokenizer(inputs, padding=True, truncation=True, return_tensors=\"pt\")[\"input_ids\"]\n",
        "\n",
        "# performs inference with a pre-trained py torch model, while making sure gradients are not calculated.\n",
        "with torch.no_grad():\n",
        "  outputs = model_saved(input_ids) # passes input_ids (input tensors) through the pre-trained model.\n",
        "\n",
        "# stores raw predictions predicted by the model.\n",
        "logits = outputs.logits\n",
        "\n",
        "# Apply softmax to obtain probabilities\n",
        "probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "# Getting the predicted labels\n",
        "predicted_labels = torch.argmax(probs, dim=-1)\n",
        "\n",
        "# maps int label to string label.\n",
        "emotion_mapping = {\n",
        "    0: \"sadness\",\n",
        "    1: \"joy\",\n",
        "    2: \"love\",\n",
        "    3: \"anger\",\n",
        "    4: \"fear\",\n",
        "    5: \"surprise\"\n",
        "}\n",
        "\n",
        "# getting the predicted emotions string labels using the mapping\n",
        "predicted_emotions = [emotion_mapping[label.item()] for label in predicted_labels]\n",
        "\n",
        "# emotions predicted in untrained state.\n",
        "print(\"Predicted emotions:\")\n",
        "print(predicted_emotions)"
      ],
      "metadata": {
        "id": "Ifgw31vtpp8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install optuna\n",
        "! pip install ray[tune]\n",
        "\n",
        "# Hyper parameter search for 10 number of trials to find the maximized accuracy\n",
        "eval = trainer.hyperparameter_search(n_trials=10, direction=\"maximize\")\n",
        "print(eval)"
      ],
      "metadata": {
        "id": "WOjJKTl9-Pn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pChmFspR5zDm"
      },
      "outputs": [],
      "source": [
        "trainer.train() # training on the dataset using trainer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating after training\n",
        "eval_test = trainer.evaluate(test_dataset)\n",
        "eval_validation = trainer.evaluate(eval_dataset)\n",
        "\n",
        "# Printing evaluated accuracy\n",
        "print(\"Testing: \", eval_test)\n",
        "print(\"Validation: \", eval_validation)"
      ],
      "metadata": {
        "id": "tyIQBZDQDPQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub()"
      ],
      "metadata": {
        "id": "aq7TXzDYRcqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trained state evaluation\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"iaminhridoy/distilbert-finetuned-emotion\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"iaminhridoy/distilbert-finetuned-emotion\")\n",
        "\n",
        "# first 5 test instances.\n",
        "inputs = test_dataset['text'][:5]\n",
        "\n",
        "# returns as py torch sequences using the tokenizer.\n",
        "input_ids = tokenizer(inputs, padding=True, truncation=True, return_tensors=\"pt\")[\"input_ids\"]\n",
        "\n",
        "# performs inference with a pre-trained py torch model, while making sure gradients are not calculated.\n",
        "with torch.no_grad():\n",
        "  outputs = model(input_ids) # passes input_ids (input tensors) through the pre-trained model.\n",
        "\n",
        "# stores raw predictions predicted by the model.\n",
        "logits = outputs.logits\n",
        "\n",
        "# Applying softmax to obtain probabilities\n",
        "probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "# Getting the predicted labels\n",
        "predicted_labels = torch.argmax(probs, dim=-1)\n",
        "\n",
        "# maps int label to string label.\n",
        "emotion_mapping = {\n",
        "    0: \"sadness\",\n",
        "    1: \"joy\",\n",
        "    2: \"love\",\n",
        "    3: \"anger\",\n",
        "    4: \"fear\",\n",
        "    5: \"surprise\"\n",
        "}\n",
        "\n",
        "# Getting the predicted emotion string labels using the mapping\n",
        "predicted_emotions = [emotion_mapping[label.item()] for label in predicted_labels]\n",
        "\n",
        "# Printing the predicted emotions\n",
        "print(\"Predicted emotions:\")\n",
        "print(predicted_emotions)"
      ],
      "metadata": {
        "id": "3sQoiSk2UNPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bert Model Text Classification"
      ],
      "metadata": {
        "id": "0HOxG5YUxqkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model = 'bert-base-uncased'"
      ],
      "metadata": {
        "id": "rgPo-Sqfrn-E"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yV8XiqKq3Wip"
      },
      "outputs": [],
      "source": [
        "# importing libaries\n",
        "\n",
        "!pip install datasets\n",
        "from datasets import load_dataset\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# Loading dataset emotion.\n",
        "dataset = load_dataset('dair-ai/emotion')\n",
        "\n",
        "# loading pre-trained model bert\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_model)\n",
        "\n",
        "# Tokenize function takes a dataset as input, pads it based on max_length and truncates if above max_length.\n",
        "def tokenize_function(example):\n",
        "  return tokenizer(example[\"text\"], padding=True, truncation=True, max_length=512)\n",
        "\n",
        "# applying tokenized function on tha dataset in batches.\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# first 5 test instances shown.\n",
        "dataset.set_format(type='pandas')\n",
        "df = dataset['test'][:5]\n",
        "print(df)"
      ],
      "metadata": {
        "id": "nRdkzOaZeit3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# loading the pre-trained weight of bert for sequence classification and initialize a model with two labels.\n",
        "def model_init():\n",
        "  return BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=6)"
      ],
      "metadata": {
        "id": "Vt7RP8LPzs1M"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_fine_tuned = \"bert-finetuned-emotion\"\n",
        "\n",
        "# Training arguments.\n",
        "training_args = TrainingArguments(\n",
        "  output_dir=bert_fine_tuned, # Directory for saving outputs\n",
        "  learning_rate=8.468317724172667e-05, # Learning rate for optimization\n",
        "  seed = 39, # num of random seeds\n",
        "  per_device_train_batch_size=64, # Batch size for training\n",
        "  per_device_eval_batch_size=64, # Batch size for evaluation\n",
        "  num_train_epochs=2, # Number of training epochs\n",
        "  weight_decay=0.01, # Weight decay for regularization\n",
        "  evaluation_strategy=\"epoch\", # Evaluation is done at the end of each epoch\n",
        "  save_strategy = \"epoch\",\n",
        ")"
      ],
      "metadata": {
        "id": "PUk3YJFT0J4w"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trainer Initialization using training pipeline from huggingface.\n",
        "\n",
        "trainer = Trainer(\n",
        "  model_init=model_init,\n",
        "  args=training_args,\n",
        "  train_dataset=tokenized_datasets['train'],\n",
        "  eval_dataset=tokenized_datasets['validation'],\n",
        "  compute_metrics=lambda p: {\"accuracy\": accuracy_score(p.label_ids,\n",
        "  np.argmax(p.predictions, axis=1))},\n",
        "  tokenizer=tokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "e59nQMpc0qko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting the test, train, and validation dataset.\n",
        "\n",
        "test_dataset = tokenized_datasets['test']\n",
        "eval_dataset = tokenized_datasets['validation']\n",
        "training_set = tokenized_datasets['train']\n",
        "\n",
        "# Untrained state evaluation.\n",
        "eval_training = trainer.evaluate(training_set)\n",
        "eval_test = trainer.evaluate(test_dataset)\n",
        "eval_validation = trainer.evaluate(eval_dataset)\n",
        "\n",
        "# Printing untrained state accuracy\n",
        "print(\"Training: \", eval_training)\n",
        "print(\"Testing: \", eval_test)\n",
        "print(\"Validation: \", eval_validation)"
      ],
      "metadata": {
        "id": "7cvY1AQZ-Hcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicted emotions before training:\n",
        "from transformers import DistilBertTokenizer, DistilBertConfig, DistilBertModel\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# loads the bert model, config and initilizes sequence classification.\n",
        "model_path = 'bert-base-uncased'\n",
        "config = DistilBertConfig.from_pretrained(model_path)\n",
        "model_saved = DistilBertForSequenceClassification.from_pretrained(model_path, config=config)\n",
        "\n",
        "# first 5 test instances.\n",
        "inputs = test_dataset['text'][:5]\n",
        "\n",
        "# returns as py torch sequences using the tokenizer.\n",
        "input_ids = tokenizer(inputs, padding=True, truncation=True, return_tensors=\"pt\")[\"input_ids\"]\n",
        "\n",
        "# performs inference with a pre-trained py torch model, while making sure gradients are not calculated.\n",
        "with torch.no_grad():\n",
        "  outputs = model_saved(input_ids) # passes input_ids (input tensors) through the pre-trained model.\n",
        "\n",
        "# stores raw predictions predicted by the model.\n",
        "logits = outputs.logits\n",
        "\n",
        "# Applying softmax to obtain probabilities\n",
        "probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "# Getting the predicted labels\n",
        "predicted_labels = torch.argmax(probs, dim=-1)\n",
        "\n",
        "# maps int label to string label.\n",
        "emotion_mapping = {\n",
        "    0: \"sadness\",\n",
        "    1: \"joy\",\n",
        "    2: \"love\",\n",
        "    3: \"anger\",\n",
        "    4: \"fear\",\n",
        "    5: \"surprise\"\n",
        "}\n",
        "\n",
        "# Getting the predicted emotions string labels using the mapping\n",
        "predicted_emotions = [emotion_mapping[label.item()] for label in predicted_labels]\n",
        "\n",
        "# Printing the predicted emotions\n",
        "print(\"Predicted emotions:\")\n",
        "print(predicted_emotions)"
      ],
      "metadata": {
        "id": "RqMPHuk0toGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install optuna\n",
        "! pip install ray[tune]\n",
        "\n",
        "# Hyper parameter search for 10 number of trials to find the maximized accuracy\n",
        "eval = trainer.hyperparameter_search(n_trials=10, direction=\"maximize\")\n",
        "print(eval)"
      ],
      "metadata": {
        "id": "tIZ0Mg-rohzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train() # training on the dataset using trainer"
      ],
      "metadata": {
        "id": "0iRUxOKk0rC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating after training\n",
        "eval_test = trainer.evaluate(test_dataset)\n",
        "eval_validation = trainer.evaluate(eval_dataset)\n",
        "\n",
        "# Printing evaluated accuracy\n",
        "print(\"Testing: \", eval_test)\n",
        "print(\"Validation: \", eval_validation)"
      ],
      "metadata": {
        "id": "gRjYdFGB06sD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# saving the model\n",
        "trainer.push_to_hub()"
      ],
      "metadata": {
        "id": "oCI4NwQp15tj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trained state evaluation\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"iaminhridoy/bert-finetuned-emotion\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"iaminhridoy/bert-finetuned-emotion\")\n",
        "\n",
        "# first 5 test instances.\n",
        "inputs = test_dataset['text'][:5]\n",
        "\n",
        "# returns as py torch sequences using the tokenizer.\n",
        "input_ids = tokenizer(inputs, padding=True, truncation=True, return_tensors=\"pt\")[\"input_ids\"]\n",
        "\n",
        "# performs inference with a pre-trained py torch model, while making sure gradients are not calculated.\n",
        "with torch.no_grad():\n",
        "  outputs = model(input_ids) # passes input_ids (input tensors) through the pre-trained model.\n",
        "\n",
        "# stores raw predictions predicted by the model.\n",
        "logits = outputs.logits\n",
        "\n",
        "# Apply softmax to obtain probabilities\n",
        "probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "# Get the predicted labels\n",
        "predicted_labels = torch.argmax(probs, dim=-1)\n",
        "\n",
        "# maps int label to string label.\n",
        "emotion_mapping = {\n",
        "    0: \"sadness\",\n",
        "    1: \"joy\",\n",
        "    2: \"love\",\n",
        "    3: \"anger\",\n",
        "    4: \"fear\",\n",
        "    5: \"surprise\"\n",
        "}\n",
        "\n",
        "# mapping the predictions with the string labels.\n",
        "predicted_emotions = [emotion_mapping[label.item()] for label in predicted_labels]\n",
        "\n",
        "# print predicted emotions\n",
        "print(\"Predicted emotions:\")\n",
        "print(predicted_emotions)"
      ],
      "metadata": {
        "id": "syHdpG6wAqKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Albert Model - Text Classification"
      ],
      "metadata": {
        "id": "-pIVjdJq16pD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "albert_model = 'albert-base-v2'"
      ],
      "metadata": {
        "id": "RTkJqljf15Yf"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing datasets and tokenizer.\n",
        "!pip install datasets\n",
        "from datasets import load_dataset\n",
        "from transformers import AlbertTokenizer\n",
        "\n",
        "# loading dataset\n",
        "dataset = load_dataset('dair-ai/emotion')\n",
        "\n",
        "# loading pre-trained model albert\n",
        "tokenizer = AlbertTokenizer.from_pretrained(albert_model)\n",
        "\n",
        "# Tokenize function takes a dataset as input, pads it based on max_length and truncates if above max_length.\n",
        "def tokenize_function(example):\n",
        "  return tokenizer(example[\"text\"], padding=True, truncation=True, max_length=512)\n",
        "\n",
        "# applying tokenized function on tha dataset in batches.\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "qwDTaxOB9hNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AlbertForSequenceClassification, Trainer, TrainingArguments\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# loading the pre-trained weight of distilbert for sequence classification and initialize a model with two labels.\n",
        "def model_init():\n",
        "  return AlbertForSequenceClassification.from_pretrained('albert-base-v2', num_labels=6)"
      ],
      "metadata": {
        "id": "STBxfYmX9kq8"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Untrained state evaluation\n",
        "\n",
        "from transformers import AlbertTokenizer, AlbertConfig, AlbertModel\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# loads the Albert model, and initilizes config, sequence classification.\n",
        "model_path = 'albert-base-v2'\n",
        "config = AlbertConfig.from_pretrained(model_path)\n",
        "model_saved = AlbertForSequenceClassification.from_pretrained(model_path, config=config)\n",
        "\n",
        "\n",
        "# first 5 test instances.\n",
        "inputs = test_dataset['text'][:5]\n",
        "\n",
        "# returns as py torch sequences using the tokenizer.\n",
        "input_ids = tokenizer(inputs, padding=True, truncation=True, return_tensors=\"pt\")[\"input_ids\"]\n",
        "\n",
        "# performs inference with a pre-trained py torch model, while making sure gradients are not calculated.\n",
        "with torch.no_grad():\n",
        "  outputs = model_saved(input_ids) # passes input_ids (input tensors) through the pre-trained model.\n",
        "\n",
        "# stores raw predictions predicted by the model.\n",
        "logits = outputs.logits\n",
        "\n",
        "# Applying softmax to obtain probabilities\n",
        "probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "# Getting the predicted labels\n",
        "predicted_labels = torch.argmax(probs, dim=-1)\n",
        "\n",
        "# maps int label to string label.\n",
        "emotion_mapping = {\n",
        "    0: \"sadness\",\n",
        "    1: \"joy\",\n",
        "    2: \"love\",\n",
        "    3: \"anger\",\n",
        "    4: \"fear\",\n",
        "    5: \"surprise\"\n",
        "}\n",
        "\n",
        "# getting the predicted emotions string labels using the mapping\n",
        "predicted_emotions = [emotion_mapping[label.item()] for label in predicted_labels]\n",
        "\n",
        "# emotions predicted in untrained state.\n",
        "print(\"Predicted emotions:\")\n",
        "print(predicted_emotions)"
      ],
      "metadata": {
        "id": "cOJDS05zpHjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "albert_fine_tuned = \"AlBert-finetuned-emotion\"\n",
        "\n",
        "# Training arguments.\n",
        "training_args = TrainingArguments(\n",
        "  output_dir=albert_fine_tuned, # Directory for saving outputs\n",
        "  learning_rate=3.069458879876956e-05, # Learning rate for optimization\n",
        "  # learning_rate': 3.069458879876956e-05, 'num_train_epochs': 5, 'seed': 22, 'per_device_train_batch_size': 32\n",
        "  seed = 22,\n",
        "  per_device_train_batch_size=16, # Batch size for training\n",
        "  per_device_eval_batch_size=16, # Batch size for evaluation\n",
        "  num_train_epochs=5, # Number of training epochs\n",
        "  weight_decay=0.01, # Weight decay for regularization\n",
        "  # load_best_model_at_end=True,\n",
        "  evaluation_strategy=\"epoch\", # Evaluation is done at the end of each epoch\n",
        "  save_strategy = \"epoch\",\n",
        ")"
      ],
      "metadata": {
        "id": "ttpsbFjR97jQ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trainer Initialization using training pipeline from huggingface.\n",
        "\n",
        "trainer = Trainer(\n",
        "  model_init=model_init,\n",
        "  args=training_args,\n",
        "  train_dataset=tokenized_datasets['train'],\n",
        "  eval_dataset=tokenized_datasets['validation'],\n",
        "  compute_metrics=lambda p: {\"accuracy\": accuracy_score(p.label_ids,\n",
        "  np.argmax(p.predictions, axis=1))},\n",
        "  tokenizer=tokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "yoADrsVs-S-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting the test, train, and validation dataset.\n",
        "\n",
        "test_dataset = tokenized_datasets['test']\n",
        "eval_dataset = tokenized_datasets['validation']\n",
        "training_set = tokenized_datasets['train']\n",
        "\n",
        "# Untrained state evaluation.\n",
        "eval_training = trainer.evaluate(training_set)\n",
        "eval_test = trainer.evaluate(test_dataset)\n",
        "eval_validation = trainer.evaluate(eval_dataset)\n",
        "\n",
        "# Printing untrained state accuracy\n",
        "print(\"Training: \", eval_training)\n",
        "print(\"Testing: \", eval_test)\n",
        "print(\"Validation: \", eval_validation)"
      ],
      "metadata": {
        "id": "x__Q5jVD_Jvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# installing libraries\n",
        "! pip install optuna\n",
        "! pip install ray[tune]\n",
        "\n",
        "# Hyper parameter search for 10 number of trials to find the maximized accuracy\n",
        "eval = trainer.hyperparameter_search(n_trials=10, direction=\"maximize\")\n",
        "print(eval)"
      ],
      "metadata": {
        "id": "I0QabOb1-VSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train() # training on the dataset using trainer"
      ],
      "metadata": {
        "id": "8Rf72Uo6-ZqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating after training\n",
        "\n",
        "eval_test = trainer.evaluate(test_dataset)\n",
        "eval_validation = trainer.evaluate(eval_dataset)\n",
        "\n",
        "# Printing evaluated accuracy\n",
        "\n",
        "print(\"Testing: \", eval_test)\n",
        "print(\"Validation: \", eval_validation)"
      ],
      "metadata": {
        "id": "nMrZKDza-cIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub() # saving the model."
      ],
      "metadata": {
        "id": "8JtRsqnT-jK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trained state evaluation\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"iaminhridoy/AlBert-finetuned-emotion\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"iaminhridoy/AlBert-finetuned-emotion\")\n",
        "\n",
        "# first 5 test instances.\n",
        "inputs = test_dataset['text'][:5]\n",
        "\n",
        "# returns as py torch sequences using the tokenizer.\n",
        "input_ids = tokenizer(inputs, padding=True, truncation=True, return_tensors=\"pt\")[\"input_ids\"]\n",
        "\n",
        "# performs inference with a pre-trained py torch model, while making sure gradients are not calculated.\n",
        "with torch.no_grad():\n",
        "  outputs = model(input_ids) # passes input_ids (input tensors) through the pre-trained model.\n",
        "\n",
        "# stores raw predictions predicted by the model.\n",
        "logits = outputs.logits\n",
        "\n",
        "# Applying softmax to obtain probabilities\n",
        "probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "# Getting the predicted labels\n",
        "predicted_labels = torch.argmax(probs, dim=-1)\n",
        "\n",
        "# maps int label to string label.\n",
        "emotion_mapping = {\n",
        "    0: \"sadness\",\n",
        "    1: \"joy\",\n",
        "    2: \"love\",\n",
        "    3: \"anger\",\n",
        "    4: \"fear\",\n",
        "    5: \"surprise\"\n",
        "}\n",
        "\n",
        "# Getting the predicted emotions string labels using the mapping\n",
        "predicted_emotions = [emotion_mapping[label.item()] for label in predicted_labels]\n",
        "\n",
        "# Printing the predicted emotions\n",
        "print(\"Predicted emotions:\")\n",
        "print(predicted_emotions)"
      ],
      "metadata": {
        "id": "eHs0qDgyAsXt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
